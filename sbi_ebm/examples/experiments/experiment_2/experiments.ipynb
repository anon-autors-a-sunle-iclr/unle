{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c61c3e43-47c5-430f-836a-ddae7cd74ed5",
   "metadata": {},
   "source": [
    "# SUNLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52653b41-d096-47fd-8682-e9bf10b9ea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments_utils import run_maybe_remotely\n",
    "from sbi_ebm.sbibm.sbi_ebm import run as run_unle\n",
    "for ns in (\n",
    "    (100,) * 10,\n",
    "    (1000,) * 10,\n",
    "    (10000,) * 10,\n",
    "):\n",
    "    for no in list(range(1, 10)):\n",
    "        for task in (\"lotka_volterra\", \"slcp\", \"two_moons\", \"gaussian_linear_uniform\"):\n",
    "            _ = run_maybe_remotely(\n",
    "                run_unle,\n",
    "                folder_name=\"iclr_experiments_2\",\n",
    "                experience_name=\"paper\",\n",
    "                use_slurm=True,\n",
    "                slurm_kwargs={\n",
    "                    \"exclude\": \"gpu-350-01,gpu-350-02,gpu-350-03,gpu-350-04,gpu-350-05,gpu-380-10,gpu-380-11,gpu-380-12,gpu-380-13,gpu-380-14,gpu-sr670-22,gpu-sr670-20\",\n",
    "                },\n",
    "                task=task,\n",
    "                num_samples=ns,\n",
    "                num_observation=no,\n",
    "                num_smc_steps=5,\n",
    "                num_mala_steps=200,\n",
    "                use_warm_start=True,\n",
    "                learning_rate=0.001 if task == \"lotka_volterra\" else 0.01,\n",
    "                max_iter=2000,\n",
    "                weight_decay=0.1,\n",
    "                random_seed=41,\n",
    "                sampler=\"mala\",\n",
    "                num_particles=1000,\n",
    "                batch_size=1000,\n",
    "                restart_every=None,\n",
    "                num_posterior_samples=10000,\n",
    "                use_nuts=False,\n",
    "                init_proposal=\"prior\",\n",
    "                # init_proposal=\"prior\",\n",
    "                noise_injection_val=0.0005,\n",
    "                # proposal=\"prior+noise\",\n",
    "                proposal=\"data\",\n",
    "                inference_sampler=\"exchange_mcmc\",\n",
    "                ebm_model_type=\"likelihood\",\n",
    "                select_based_on_test_loss=False,\n",
    "                inference_proposal=\"prior\",\n",
    "                use_data_from_past_rounds=True,\n",
    "                inference_num_warmup_steps=2000,\n",
    "                exchange_mcmc_inner_sampler_num_steps=100,\n",
    "                evaluate_posterior=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b3ef8b-9b13-434b-80bf-54adea8fd691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments_utils import run_maybe_remotely\n",
    "from sbi_ebm.sbibm.sbi_ebm import run as run_unle\n",
    "for ns in (\n",
    "    (1000,),\n",
    "    (10000,),\n",
    "    (100000,),\n",
    "):\n",
    "    for no in list(range(1, 10)):\n",
    "        for task in (\"slcp\", \"two_moons\", \"gaussian_linear_uniform\", \"lotka_volterra\"):\n",
    "            _ = run_maybe_remotely(\n",
    "                run_unle,\n",
    "                folder_name=\"iclr_experiments_2\",\n",
    "                experience_name=\"paper2\",\n",
    "                use_slurm=True,\n",
    "                slurm_kwargs={\n",
    "                    \"exclude\": \"gpu-350-01,gpu-350-02,gpu-350-03,gpu-350-04,gpu-350-05,gpu-380-10,gpu-380-11,gpu-380-12,gpu-380-13,gpu-380-14\",\n",
    "                },\n",
    "                # \"two_moons\", (1000,), 1,\n",
    "                task=task,\n",
    "                num_samples=ns,\n",
    "                num_observation=no,\n",
    "                num_smc_steps=20,\n",
    "                num_mala_steps=3,\n",
    "                use_warm_start=True,\n",
    "                learning_rate=0.01,\n",
    "                max_iter=2000,\n",
    "                weight_decay=0.1,\n",
    "                random_seed=40,\n",
    "                sampler=\"smc\",\n",
    "                num_particles=1000,\n",
    "                batch_size=1000,\n",
    "                restart_every=None,\n",
    "                num_posterior_samples=10000,\n",
    "                use_nuts=False,\n",
    "                init_proposal=\"prior\",\n",
    "                # init_proposal=\"prior\",\n",
    "                noise_injection_val=0.0005,\n",
    "                proposal=\"prior+noise\",\n",
    "                # proposal=\"data\",\n",
    "                inference_sampler=\"smc\",\n",
    "                ebm_model_type=\"joint_tilted\",\n",
    "                select_based_on_test_loss=False,\n",
    "                inference_proposal=\"prior\",\n",
    "                use_data_from_past_rounds=False,\n",
    "                evaluate_posterior=True\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unle",
   "language": "python",
   "name": "unle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
